# ğŸ“š Essential ML Research Papers and Book Chapters (Topic-wise)

This guide complements your YouTube/self-learning path with **must-read** academic resources and textbook references. Only critical topics are listed.

---

## ğŸ“˜ Supervised Learning

### ğŸ“— Book: *The Elements of Statistical Learning* (ESL) â€“ Hastie, Tibshirani, Friedman
- Chapter 2 â€“ Linear Regression
- Chapter 3 â€“ Feature Selection & Shrinkage Methods
- Chapter 4 â€“ Classification (Logistic Regression, LDA)
- Chapter 9 â€“ Additive Models, Trees, and Related Methods
- Chapter 10 â€“ Boosting and Additive Trees
- Chapter 7 â€“ Model Assessment and Selection

### ğŸ“— Book: *Pattern Recognition and Machine Learning* â€“ Christopher Bishop  
- Ch. 3 â€“ Naive Bayes, Multinomial Logistic Regression  
- Ch. 4 â€“ SVM concepts (kernel methods, margin)

### ğŸ“„ Paper: **A Few Useful Things to Know About Machine Learning** â€“ Pedro Domingos  
â†’ [Link](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)

### ğŸ“„ Paper: **Understanding the Bias-Variance Tradeoff** â€“ Scott Fortmann-Roe  
â†’ [Link](https://scott.fortmann-roe.com/docs/BiasVariance.html)

### ğŸ“„ Paper: **Support-Vector Networks** â€“ Cortes & Vapnik, 1995  
â†’ [Link](https://link.springer.com/article/10.1007/BF00994018)

### ğŸ“„ Paper: **Random Forests** â€“ Leo Breiman, 2001  
â†’ [Link](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)

---

## ğŸ“Š Ensemble Methods

### ğŸ“„ Paper: **The Strength of Weak Learnability** â€“ Robert Schapire (1990)  
â†’ [Link](https://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf)

### ğŸ“„ Paper: **XGBoost: A Scalable Tree Boosting System** â€“ Chen & Guestrin (2016)  
â†’ [Link](https://arxiv.org/abs/1603.02754)

---

## ğŸ“™ Unsupervised Learning

### ğŸ“— Book: *Pattern Recognition and Machine Learning* â€“ Bishop  
- Ch. 9 â€“ Mixture Models and EM Algorithm  
- Ch. 12 â€“ Sequential Data & Time Series

---

## ğŸ“Š Clustering

### ğŸ“„ Paper: *A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise (DBSCAN)* â€“ Ester et al., 1996  
â†’ [Link](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf)

### ğŸ“„ Paper: *Spectral Clustering Tutorial* â€“ von Luxburg, 2007  
â†’ [Link](https://arxiv.org/abs/0711.0189)

### ğŸ“„ Paper: *Clustering by Gaussian Mixture Models* â€“ Dempster et al., 1977  
â†’ [Link](https://www.jstor.org/stable/2984875)

---

## ğŸ“ Dimensionality Reduction

### ğŸ“„ Paper: *A Tutorial on Principal Component Analysis* â€“ Lindsay Smith  
â†’ [Link](https://arxiv.org/pdf/1404.1100)

### ğŸ“„ Paper: *Visualizing Data using t-SNE* â€“ van der Maaten & Hinton  
â†’ [Link](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)

### ğŸ“„ Paper: *UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction* â€“ McInnes et al., 2018  
â†’ [Link](https://arxiv.org/abs/1802.03426)

---

## ğŸ“¦ Association Rule Learning

### ğŸ“„ Paper: *Fast Algorithms for Mining Association Rules* â€“ Agrawal & Srikant, 1994  
â†’ [Link](http://www.vldb.org/conf/1994/P487.PDF)

### ğŸ“„ Paper: *FP-Growth: Mining Frequent Patterns without Candidate Generation* â€“ Han et al., 2000  
â†’ [Link](https://doi.org/10.1145/335191.335372)

---

## âš ï¸ Anomaly Detection

### ğŸ“„ Paper: *Isolation Forest* â€“ Liu, Ting, Zhou (2008)  
â†’ [Link](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)

### ğŸ“„ Paper: *A Survey of Outlier Detection Methodologies* â€“ Chandola et al., 2009  
â†’ [Link](https://www.cs.umn.edu/sites/cs.umn.edu/files/tech_reports/07-017.pdf)

---

## â³ Time Series Clustering

### ğŸ“„ Paper: *Time-Series Clustering â€“ A Decade Review* â€“ Aghabozorgi et al., 2015  
â†’ [Link](https://doi.org/10.1016/j.ins.2014.09.043)

### ğŸ“„ Paper: *Dynamic Time Warping Algorithm Review* â€“ MÃ¼ller, 2007  
â†’ [Link](https://dl.acm.org/doi/10.1145/1187415.1187417)

---

## ğŸ§  Model Interpretability

### ğŸ“„ Paper: *A Unified Approach to Interpreting Model Predictions (SHAP)* â€“ Lundberg & Lee  
â†’ [Link](https://arxiv.org/abs/1705.07874)

### ğŸ“„ Paper: *"Why Should I Trust You?" Explaining the Predictions of Any Classifier (LIME)* â€“ Ribeiro et al.  
â†’ [Link](https://arxiv.org/abs/1602.04938)

---

## âš™ï¸ Feature Engineering & Imbalance Handling

### ğŸ“„ Paper: *SMOTE: Synthetic Minority Over-sampling Technique* â€“ Chawla et al., 2002  
â†’ [Link](https://arxiv.org/abs/1106.1813)

### ğŸ“„ Paper: *Feature Selection: A Data Perspective* â€“ Li et al., 2017  
â†’ [Link](https://arxiv.org/abs/1601.07996)

---

## ğŸ§ª Experimentation & Optimization

### ğŸ“„ Paper: *Data Leakage in ML* â€“ Towards Data Science  
â†’ [Link](https://towardsdatascience.com/data-leakage-in-machine-learning-what-is-it-and-how-to-avoid-it-96f1e5efac0b)

### ğŸ“˜ *Deep Learning* â€“ Ian Goodfellow et al.  
- Ch. 7 â€“ Optimization and Early Stopping  
- Ch. 14 â€“ Autoencoders and Representation Learning

---

### âœ… How to Use This

- Start with **book chapters** for theory  
- Refer to **papers** for implementation-critical or complex topics  
- Check these off in your branchâ€™s `topics.md` file to track coverage

---

## ğŸ§  Bonus Book (Practical Focus)

### ğŸ“˜ *Hands-On Unsupervised Learning Using Python* â€“ Ankur Patel  
- Easy-to-follow with implementation examples  
- Covers clustering, dimensionality reduction, anomaly detection

---

## ğŸ¤– Reinforcement Learning

### ğŸ“— Book: *Reinforcement Learning: An Introduction* â€“ Sutton & Barto (2nd Edition)  
â†’ [Link (free)](http://incompleteideas.net/book/the-book-2nd.html)

- Chapter 1 â€“ Intro
- Chapter 3 â€“ Finite Markov Decision Processes
- Chapter 4 â€“ Dynamic Programming
- Chapter 5 â€“ Monte Carlo Methods
- Chapter 6 â€“ Temporal-Difference Learning
- Chapter 9 â€“ On-policy and Off-policy Learning (SARSA vs Q-Learning)
- Chapter 13 â€“ Policy Gradient Methods
- Appendix: For math review

### ğŸ“„ Paper: **Playing Atari with Deep Reinforcement Learning** â€“ DeepMind (DQN)  
â†’ [Link](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  
- Read Abstract, Intro, and Sections 3â€“5 for architecture and results.

### ğŸ“„ Paper: **Proximal Policy Optimization Algorithms** â€“ Schulman et al. (OpenAI)  
â†’ [Link](https://arxiv.org/abs/1707.06347)  
- Read Sections 1â€“3 and Appendix for core PPO logic.

---

## ğŸ§ª Meta-Learning & Experimentation

### ğŸ“„ Paper: **Data Leakage in ML** â€“ Towards Data Science (Practical Read)  
â†’ [Link](https://towardsdatascience.com/data-leakage-in-machine-learning-what-is-it-and-how-to-avoid-it-96f1e5efac0b)

---

### âœ… How to Use This

- Start with **book chapters** for fundamental theory
- Use **papers** when working on implementation-heavy or research-driven projects
- Combine this with **YouTube + coding practice** (e.g., Kaggle, scikit-learn, PyTorch)

---

Happy Learning & Experimenting ğŸš€  
Feel free to fork this roadmap and add more papers per use-case!

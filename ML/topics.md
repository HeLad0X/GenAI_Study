# âœ… Machine Learning Topics Checklist

Use this checklist to track your progress through the full ML journey â€” from foundations to deployment-ready models.

---

## ğŸŸ¢ 1. Core Foundations (Math + Tools)

- [x] Linear Algebra: Vectors, Matrices, Dot Products, Inverses
- [x] Probability & Statistics: Bayes' Rule, Expectation, Variance, Distributions
- [x] Calculus: Derivatives, Gradients, Chain Rule
- [x] Python Basics + Numpy
- [x] `pandas`, `matplotlib`, `cupy`, `cudf` basics

---

## ğŸŸ¡ 2. Working with Data

### ğŸ› ï¸ Preprocessing & Feature Engineering
- [x] Handling Missing Data âœ…
- [x] Categorical Encoding (One-Hot, Label) âœ…
- [x] Feature Scaling (Z-score, IQR, Log) âœ…
- [ ] Feature Selection (Variance Threshold, Correlation)
- [ ] Outlier Detection & Handling
- [x] Skewness & Normalization âœ…
- [ ] Handling Class Imbalance (SMOTE, Undersampling)

---

## ğŸ§ª 3. Model Evaluation & Metrics

- [x] Mean Squared Error (MSE) âœ…
- [x] Root Mean Squared Error (RMSE) âœ…
- [x] Mean Absolute Error (MAE) âœ…
- [x] RÂ² and Adjusted RÂ² âœ…
- [ ] Accuracy, Precision, Recall, F1-score
- [ ] Confusion Matrix
- [ ] ROC-AUC Curve, PR Curve
- [ ] Log Loss
- [ ] Kappa Score (optional)

---

## ğŸ”µ 4. Supervised Learning

### ğŸ”¹ Regression
- [x] Linear Regression (Normal Equation)
- [x] Linear Regression (Gradient Descent)
- [ ] Polynomial Regression âœ…
- [ ] Ridge & Lasso Regression (L1/L2 Regularization)
- [x] Logistic Regression (Binary Classification) âœ…
- [ ] Multinomial Logistic Regression

### ğŸ”¹ Classification
- [ ] K-Nearest Neighbors (KNN)
- [ ] Decision Trees
- [ ] Random Forests
- [ ] Naive Bayes
- [ ] Support Vector Machines (SVM)

---

## ğŸŸ£ 5. Model Selection & Hyperparameter Tuning

- [ ] Bias-Variance Tradeoff
- [ ] Cross-Validation (K-Fold, Stratified K-Fold) âœ… `logistic_regression.py`
- [x] Grid Search âœ… `cuml_linear_regression.py`
- [ ] Random Search
- [ ] Early Stopping (for iterative models)

---

## ğŸŸ¤ 6. Unsupervised Learning (Expanded)

### ğŸ“Š Clustering
- [ ] K-Means Clustering
- [ ] Hierarchical Clustering (Agglomerative & Divisive)
- [ ] DBSCAN
- [ ] Gaussian Mixture Models (GMM)
- [ ] Spectral Clustering (optional)

### ğŸ“ Dimensionality Reduction
- [ ] Principal Component Analysis (PCA)
- [ ] t-SNE (T-distributed Stochastic Neighbor Embedding)
- [ ] UMAP (Uniform Manifold Approximation)
- [ ] Autoencoders (Intro)

### ğŸ” Association Rule Learning
- [ ] Apriori Algorithm
- [ ] Eclat Algorithm
- [ ] FP-Growth Algorithm

### âš ï¸ Anomaly/Outlier Detection
- [ ] Z-Score / IQR Method (basic stats-based)
- [ ] Isolation Forest
- [ ] One-Class SVM
- [ ] DBSCAN (used for outlier detection too)

### â³ Time Series (Optional Intro)
- [ ] Time Series Decomposition (Trend, Seasonality)
- [ ] Clustering Time Series (Dynamic Time Warping)

---

## ğŸ”´ 7. Ensemble Methods

- [ ] Bagging (e.g., Random Forests revisit)
- [ ] Boosting (AdaBoost, Gradient Boosting)
- [ ] XGBoost
- [ ] LightGBM
- [ ] Stacking & Blending

---

## âš« 8. Deployment, Pipelines, and Extras

- [x] Saving/Loading Models (`joblib`, `pickle`) âœ… used in all models
- [ ] Building ML Pipelines (e.g., `Pipeline` API) âœ… `cuml_linear_regression.py`
- [ ] Real-world Datasets: Iris, Titanic, Diabetes âœ… `config.py` supports
- [ ] Basic CI/CD for ML (optional)
- [ ] Introduction to DVC (Data Version Control)
- [ ] Model Monitoring Concepts

---

## ğŸ§  9. Interpretability (Optional but Important)

- [ ] Feature Importance (for Trees & Linear models)
- [ ] SHAP (SHapley Additive explanations)
- [ ] LIME (Local Interpretable Model-agnostic Explanations)

---

ğŸ“Œ *Check `[ ]` to `[x]` as you progress. All âœ… symbols show where your current codebase already covers key topics.*
